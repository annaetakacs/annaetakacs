---
title: "Harvard Machine Learning W1"
author: "Anna Takacs"
date: "12/27/2019"
output:
  pdf_document: default
  html_document: default
---
```{r include=FALSE}
library(caret)
library(dslabs)
library(dplyr)
```

# Overall Accuracy 
We have only one predictor and so, y is sex and x is height. 
```{r}
data(heights)
y <- heights$sex
x <- heights$height
```

Randomly split the data into training and test sets. 

- The argument `times` is used to define how many random samples of indexes to return.

- The agrument `p` is used to define what proportion of the index represented.

- Angument `list` is used to decide what indexes to be returned as a list of not.

```{r}
set.seed(2)
test_index <- createDataPartition(y, times = 1, p = 0.5, list = FALSE)
test_set <- heights[test_index, ]
train_set <- heights[-test_index, ]
```

### Overall accuracy: 
_Evaluating algorithms when the outcome is categorical is simply by reproting the proportion of cases that were correcty predicted in the test set._

To be able to demonstarte the use of overall accuracy, we build two competing algorithms and compare them: 

__1) Guessing the outcome.__

Here, were ignoring the predictor and simply guessing the dependent variable. 

```{r}
y_hat <- sample(c("Male", "Female"), length(test_index), replace = TRUE)
```

The `caret` package recommend categorical variables to be decoded as factors. 
```{r}
y_hat <- sample(c("Male", "Female"), length(test_index), replace = TRUE) 
y_hat <- y_hat %>% 
  factor(levels = levels(test_set$sex))
```

Compute overall accuracy by showing the overall proportion that is predicted correctly: 
```{r}
mean(y_hat == test_set$sex)
```

BUT: we can do better because from the explanatory data analysis we can see that men are slighlty taller than females.

```{r}
heights %>% group_by(sex) %>% summarize(mean(height), sd(height))
```

__2) Model 2__

In this model we try a simple approach: predict male if the height is within two standard devatition from the average male. The accuracy goes up to around 80% from the previous 50%. 

```{r}
y_hat <- ifelse(x > 62, "Male", "Female") %>% 
  factor(levels = levels(test_set$sex))
mean(y == y_hat)
```

Here, we picked the cutoff at 62, but we can pick the best value on the training set. BUT: evaluating algorhithms on the training set can lead to overfitting - can lead to overly optimistic assessments. 

Now, we examine the results with 10 different cutoffs. 

```{r}
library(purrr)
cutoff <- seq(61, 70)
accuracy <- map_dbl(cutoff, function(x){
        y_hat <- ifelse(train_set$height > x, "Male", "Female") %>% 
                factor(levels = levels(test_set$sex))
        mean(y_hat == train_set$sex)
})
```

The following plot shows accuracy for females and males.
```{r}
data.frame(cutoff, accuracy) %>% 
     ggplot(aes(cutoff, accuracy)) + 
     geom_point() + 
     geom_line() 
```

The maximum value for accuracy is much higher than the previous 50%. 
See where we can achieve maximized accuracy: 

```{r}
max(accuracy)
best_cutoff <- cutoff[which.max(accuracy)]
best_cutoff
```

The accuracy is maximised at the 65 cutoff. 

__Test the cutoff on the test set.__
```{r}
y_hat <- ifelse(test_set$height > best_cutoff, "Male", "Female") %>% 
     factor(levels = levels(test_set$sex))
y_hat <- factor(y_hat)
mean(y_hat == test_set$sex)
```

Here, the accuracy is around 81%. It is still better than guessing and we know that it's not due to overfitting as we tested this on the testing dataset. 

## Exercises

How many features are available to us for prediction in the mnist digits dataset?
You can download the mnist dataset using the read_mnist() function from the dslabs package.

```{r error = TRUE}
#install.packages("dslabs")
library(dslabs)
mnist <- read_mnist()
mnist_x <- mnist$train$images
mnist_y <- mnist$train$labels

# Rename columns and standardize feature values
colnames(mnist_x) <- paste0("V", 1:ncol(mnist_x))
mnist_x <- mnist_x / 255

#One-hot encode response 
mnist <- as.character(mnist_y, 10)

# Get number of features, which we'll use in our model. 
p <- ncol(mnist_x)
p
```

_Feature_ is an individual measurable property or characteristic of a phenomenon being observed.

# Confusion Matrix 

Overall accuracy can be a deciptive measure. -> To see this, we start constructing the confusion matrix - whihc tabulates each combination of prediction and actual value. 

By using the table() function: 
```{r}
table(predicted = y_hat, actual = test_set$sex)
test_set %>% 
     mutate(y_hat = y_hat) %>%
     group_by(sex) %>% 
     summarize(accuracy = mean(y_hat == sex))
```
We get a very high accuracy for males, but a lower accuracy for females. In other words, too many females are predicted to be males. 

_How can the overall accuracy be so high then?_ Because there are more males in the datatset than females. 
```{r}
prev <- mean(y == "Male")
prev
```
77% of the individuals in the dataset are males. 

__If the training data is biased in some way, you are likely to develop an algorhithm that is biased as well.__
-> Look at metrics other than overall accuracy when evaluating a machine learning algorhithm.

Metrics to evaluate an algorhithm where prevelance does not cloud our assessment: 
Can be derived from the confusion matrix: allow us to study sensitivity and specificity separately. 
In case of a categorical outcome, we can define sensitivity and specificity for a specific category. 

_Sensitivity_: ability of an algorhithm to predict a positive outcome when the actual outcome is positive. 

_Specificity_: the ability to NOT predict the positive when the actual value is not a positive.

High sensitivity: y = 1 implies y hat = 1

High specificity: y = 0 implies y hat = 0

__Confusion matrix:__
```{r}
mat <- matrix(c("True positives (TP)", "False negatives (FN)", 
                "False positives (FP)", "True negatives (TN)"), 2, 2)
colnames(mat) <- c("Actually Positive", "Actually Negative")
rownames(mat) <- c("Predicted positve", "Predicted negative")
as.data.frame(mat) %>% knitr::kable()
```

__Sensitivity:__
  True positive rate/ recall: TP/(TP+FN)

__Specificity:__
  True negative rate: TN/(TN+FP)
  
  Positive predictive value/ precision: TP/(TP+FP) (It depends on the prevalance, since higher prevalence implies you can get higher precision even when guessing)
  
### Confusion matrix
We need to define what is positive. The function expects factors as inputs and the first level is considered the positive outcome y = 1.

In out example female is the first level as it comes before male alphabetically. 
```{r}
confusionMatrix(data = y_hat, reference = test_set$sex)
```

High overall accuracy is possible despite relatively low sensitivity. It happens because of the low prevalence. 

-> Because prevalence is low, failing to call actual females females, LOW SENSITIVITY, does not lower the accuracy as much as it woulf have increased if incorrectly called males females. 

# Balanced accuracy and F-score
Balanced accuracy: average of specificity and sensitivity (both of which are rates) 

__F1 score__: harmonic average of precision and recall 

_2 x (precision x recall)/(precision + recall)_

Depending on the context: some type of errors are more costly than others. 

--> Plane safety: maximize sensitivity over specificity. 

--> F1 can be adopted to weight specificity and sesitivity: we define `beta` tp represent how much more important sensitivity is compared ot specificity. 

__Caret package__: F_meas() fucntion - computes the summary with beta defaulting to one 

### Maximizing F score instead of overall accuracy 
```{r}
cutoff <- seq(61, 70)
F_1 <- map_dbl(cutoff, function(x){
     y_hat <- ifelse(train_set$height > x, "Male", "Female") %>% 
          factor(levels = levels(test_set$sex))
     F_meas(data = y_hat, reference = factor(train_set$sex))
})
```

```{r}
data.frame(cutoff, F_1) %>% 
     ggplot(aes(cutoff, F_1)) + 
     geom_point() + 
     geom_line()
max(F_1)
```

```{r}
best_cutoff <- cutoff[which.max(F_1)]
best_cutoff
```
The cutoff of 66 makes much more sense. 

And this result also balances the specificity and sentsitivity of our confusion matrix: 
```{r}
y_hat <- ifelse(test_set$height > best_cutoff, "Male", "Female") %>% 
     factor(levels = levels(test_set$sex))
confusionMatrix(data = y_hat, reference = test_set$sex)
```
We do much better than guessing and both sensitivity and specificity are relatively high. 

This is our first machine learning algorithm: takes hight as a predictor and predicts female if you are 66 inches or shorter. 

# Prevalence matters in practice
Machine learning algorhythm with high sensitivity and specificity: may not be useful in practice when prevalence is close to 1 or 0. 

__Case__: Doctor who wants to predicts the rare disease, data has 50% of people who have the disease. Meaning that Y_hat equals 1/2. 

But here, what is important is the precision of the test - the probability of y equals 1 given that Y_hat equals 1. 

We can apply here Bayes' theorem. 

The doctor knwos that the prevalence of the disease is 5 in 1000. And the prevalence of the disease in the dataset is 50%. 

And so: 

Pr(y = 1) / Pr(Y_hat = 1) = 1/100 

And so: the precision of the algorhythm is less than 0.01. The doctor cannot do much with the algothythm. 

# ROC and precision-recall curves
Accuracy: only considered one cutoff - here we only considered with equal probability. 

F1: one that used height and considered several cutoffs - clearly outperformed. 

Here, guessing male with higher probability would give us higher accuracy due to bias in sample. This code prdicts male. 
```{r}
p <- 0.9
y_hat <- sample(c("Male", "Female"), length(test_index), replace = TRUE, prob=c(p, 1-p)) %>% 
     factor(levels = levels(test_set$sex))
mean(y_hat == test_set$sex)
```
By guessing male 90% of the time, the accuracy goes up to 0.72%. 
But this would come at a cost of lower sensitivity. 

A very common approach to evaluating methods is to compare them graphically by plotting both: the receiver operating characteristic or ROC. 

ROC + true positive rate __OR__ 1-false positive rate. 

ROC curve for guessing sex but using different probabilities of guessing male: 
```{r}
probs <- seq(0, 1, length.out = 10)
guessing <- map_df(probs, function(p){
     y_hat <- 
          sample(c("Male", "Female"), length(test_index), replace = TRUE, prob=c(p, 1-p)) %>% 
          factor(levels = c("Female", "Male"))
     list(method = "Guessing",
          FPR = 1 - specificity(y_hat, test_set$sex),
          TPR = sensitivity(y_hat, test_set$sex))
})
guessing %>% qplot(FPR, TPR, data =., xlab = "1 - Specificity", ylab = "Sensitivity")
```

The perfect algorhythm would shoot straight to one and stay up there, meaning perfect sensitivity for all values of specificity. 

Construct an ROC curve for a height-based approach: 
```{r}
cutoffs <- c(50, seq(60, 75), 80)
height_cutoff <- map_df(cutoffs, function(x){
     y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
          factor(levels = c("Female", "Male"))
     list(method = "Height cutoff",
          FPR = 1-specificity(y_hat, test_set$sex),
          TPR = sensitivity(y_hat, test_set$sex))
})
```

Plotting both curves together, we are able to compare sensitivity for different values of specificity: 
```{r}
bind_rows(guessing, height_cutoff) %>%
     ggplot(aes(FPR, TPR, color = method)) +
     geom_line() +
     geom_point() +
     xlab("1 - Specificity") +
     ylab("Sensitivity")
```

We obtain higher sensitivity with the height-based approach for all values of specificiity- which implies that it is a better method. 

Adding cutoff to the points: 
```{r}
map_df(cutoffs, function(x){
     y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
          factor(levels = c("Female", "Male"))
     list(method = "Height cutoff",
          cutoff = x, 
          FPR = 1-specificity(y_hat, test_set$sex),
          TPR = sensitivity(y_hat, test_set$sex))
}) %>%
     ggplot(aes(FPR, TPR, label = cutoff)) +
     geom_line() +
     geom_point() +
     geom_text(nudge_y = 0.01)
```

ROC has one weakness: neither of the measures depend on prevalence. 

In cases where pevalence matters we may make a precision-recall plot. - Plots precision over recall. 
```{r message=FALSE, warning=FALSE}
guessing <- map_df(probs, function(p){
     y_hat <- sample(c("Male", "Female"), length(test_index), 
                     replace = TRUE, prob=c(p, 1-p)) %>% 
          factor(levels = c("Female", "Male"))
     list(method = "Guess",
          recall = sensitivity(y_hat, test_set$sex),
          precision = precision(y_hat, test_set$sex))
})
height_cutoff <- map_df(cutoffs, function(x){
     y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
          factor(levels = c("Female", "Male"))
     list(method = "Height cutoff",
          recall = sensitivity(y_hat, test_set$sex),
          precision = precision(y_hat, test_set$sex))
})
bind_rows(guessing, height_cutoff) %>%
     ggplot(aes(recall, precision, color = method)) +
     geom_line() +
     geom_point()
```

Precision of guessing is not high. This is because the prevalence is low. 

If we change positives to mean male instead of females, the ROC curve remains the same - but the precision-recall plot changes. 
```{r message=FALSE, warning=FALSE}
guessing <- map_df(probs, function(p){
     y_hat <- sample(c("Male", "Female"), length(test_index), replace = TRUE, 
                     prob=c(p, 1-p)) %>% 
          factor(levels = c("Male", "Female"))
     list(method = "Guess",
          recall = sensitivity(y_hat, relevel(test_set$sex, "Male", "Female")),
          precision = precision(y_hat, relevel(test_set$sex, "Male", "Female")))
})
height_cutoff <- map_df(cutoffs, function(x){
     y_hat <- ifelse(test_set$height > x, "Male", "Female") %>% 
          factor(levels = c("Male", "Female"))
     list(method = "Height cutoff",
          recall = sensitivity(y_hat, relevel(test_set$sex, "Male", "Female")),
          precision = precision(y_hat, relevel(test_set$sex, "Male", "Female")))
})
bind_rows(guessing, height_cutoff) %>%
     ggplot(aes(recall, precision, color = method)) +
     geom_line() +
     geom_point()
```

## Exercises 
__Question 1__
```{r}
library(dslabs)
library(dplyr)
library(lubridate)
data("reported_heights")
```

```{r}
dat <- mutate(reported_heights, date_time = ymd_hms(time_stamp)) %>%
  filter(date_time >= make_date(2016, 01, 25) & date_time < make_date(2016, 02, 1)) %>%
  mutate(type = ifelse(day(date_time) == 25 & hour(date_time) == 8 & between(minute(date_time), 15, 30), "inclass","online")) %>%
  select(sex, type)

y <- factor(dat$sex, c("Female", "Male"))
x <- dat$type
```

```{r}
library(dplyr)
dat_inclass <- dat %>% 
  filter(dat$type == "inclass")
table <- table(dat_inclass$sex)
round(prop.table(table), 2)

dat_online <- dat %>% 
  filter(dat$type == "online")
table1 <- table(dat_online$sex)
round(prop.table(table1), 2)
```

__Question 2__
```{r}
y_hat <- ifelse(x == "online", "Male", "Female") %>% 
  factor(levels = levels(y))
mean(y == y_hat)
```

__Question 3___
```{r}
table(y_hat, y)
```

__Question 4__
```{r}
sensitivity(y_hat, y)
specificity(y_hat, y)
```

__Question 5__
```{r}
sex <- table(dat$sex)
prop.table(sex)
```

__Question 7__
```{r}
library(caret)
data(iris)
iris <- iris[-which(iris$Species=='setosa'),]
y <- iris$Species
```

```{r message=FALSE, warning=FALSE}
set.seed(2, sample.kind = "Rounding")  
test_index <- createDataPartition(y, times = 1,p = 0.5, list = FALSE)
test <- iris[test_index,]
train <- iris[-test_index,]
```

__Question 8__

Sepal.Length
```{r eval=FALSE, include=FALSE}
y_hat <- sample(c("virginica", "versicolor"), length(test_index), replace = TRUE) 

library(purrr)
cutoff <- seq(4.9, 7.9, 0.1)
accuracy <- map_dbl(cutoff, function(x){
        y_hat <- ifelse(train$Sepal.Length > x, "virginica", "versicolor") %>%
          factor(levels = levels(train$Species))
        mean(y_hat == train$Species)
})

data.frame(cutoff, accuracy) %>% 
     ggplot(aes(cutoff, accuracy)) + 
     geom_point() + 
     geom_line() 

max(accuracy)
```

Sepal.Width
```{r eval=FALSE, include=FALSE}
y_hat <- sample(c("virginica", "versicolor"), length(test_index), replace = TRUE) 

cutoff <- seq(2.0, 3.8, 0.1)
accuracy <- map_dbl(cutoff, function(x){
        y_hat <- ifelse(train$Sepal.Width > x, "virginica", "versicolor") %>%
          factor(levels = levels(train$Species))
        mean(y_hat == train$Species)
})

data.frame(cutoff, accuracy) %>% 
     ggplot(aes(cutoff, accuracy)) + 
     geom_point() + 
     geom_line() 

max(accuracy)
```

Petal.Length
```{r}
y_hat <- sample(c("virginica", "versicolor"), length(test_index), replace = TRUE) 

cutoff <- seq(1.0, 6.9, 0.1)
accuracy <- map_dbl(cutoff, function(x){
  y_hat <- ifelse(train$Petal.Length > x, "virginica", "versicolor") %>%
          factor(levels = levels(train$Species))
        mean(y_hat == train$Species)
})

data.frame(cutoff, accuracy) %>% 
     ggplot(aes(cutoff, accuracy)) + 
     geom_point() + 
     geom_line() 

max(accuracy)
```

Petal.Width
```{r eval=FALSE, include=FALSE}
y_hat <- sample(c("virginica", "versicolor"), length(test_index), replace = TRUE) 

cutoff <- seq(1.0, 2.5, 0.1)
accuracy <- map_dbl(cutoff, function(x){
  y_hat <- ifelse(train$Petal.Width > x, "virginica", "versicolor") %>%
          factor(levels = levels(train$Species))
        mean(y_hat == train$Species)
})

data.frame(cutoff, accuracy) %>% 
     ggplot(aes(cutoff, accuracy)) + 
     geom_point() + 
     geom_line() 

max(accuracy)
```

The highest accuracy is for the Petal.Length variable.

__Question 9__
Calculate overall accuracy in the test data.
```{r}
best_cutoff <- cutoff[which.max(accuracy)]
best_cutoff
```

```{r}
y_hat <- ifelse(test$Petal.Length > best_cutoff, "virginica", "versicolor") %>%
          factor(levels = levels(test$Species))
        mean(y_hat == test$Species)
```

__Question 10__
```{r}
# Sepal.Length
y_hat <- sample(c("virginica", "versicolor"), length(test_index), replace = TRUE) 

library(purrr)
cutoff <- seq(4.9, 7.9, 0.1)
accuracy <- map_dbl(cutoff, function(x){
        y_hat <- ifelse(train$Sepal.Length > x, "virginica", "versicolor") %>%
          factor(levels = levels(train$Species))
        mean(y_hat == train$Species)
})
max(accuracy)
# Max overall accuracy 
best_cutoff <- cutoff[which.max(accuracy)]
best_cutoff
y_hat <- ifelse(test$Sepal.Length > best_cutoff, "virginica", "versicolor") %>%
          factor(levels = levels(test$Species))
        mean(y_hat == test$Species)
```
```{r}
# Sepal Width
y_hat <- sample(c("virginica", "versicolor"), length(test_index), replace = TRUE) 

cutoff <- seq(2.0, 3.8, 0.1)
accuracy <- map_dbl(cutoff, function(x){
        y_hat <- ifelse(train$Sepal.Width > x, "virginica", "versicolor") %>%
          factor(levels = levels(train$Species))
        mean(y_hat == train$Species)
})

max(accuracy)
# Max overall accuracy 
best_cutoff <- cutoff[which.max(accuracy)]
best_cutoff
y_hat <- ifelse(test$Sepal.Width > best_cutoff, "virginica", "versicolor") %>%
          factor(levels = levels(test$Species))
        mean(y_hat == test$Species)
```

```{r}
# Petal Width
y_hat <- sample(c("virginica", "versicolor"), length(test_index), replace = TRUE) 

cutoff <- seq(1.0, 2.5, 0.1)
accuracy <- map_dbl(cutoff, function(x){
  y_hat <- ifelse(train$Petal.Width > x, "virginica", "versicolor") %>%
          factor(levels = levels(train$Species))
        mean(y_hat == train$Species)
})

max(accuracy)
# Max overall accuracy 
best_cutoff <- cutoff[which.max(accuracy)]
best_cutoff
y_hat <- ifelse(test$Petal.Width > best_cutoff, "virginica", "versicolor") %>%
          factor(levels = levels(test$Species))
        mean(y_hat == test$Species)
```

__Question 11__
```{r}
plot(iris, pch = 21,bg = iris$Species)
```

Optimize cutoff separately in the training dataset for Petal.Length and Petal.Width
Petal.Length
```{r}
y_hat <- sample(c("virginica", "versicolor"), length(test_index), replace = TRUE) 

cutoff <- seq(1.0, 6.9, 0.1)
accuracy <- map_dbl(cutoff, function(x){
  y_hat <- ifelse(train$Petal.Length > x, "virginica", "versicolor") %>%
          factor(levels = levels(train$Species))
        mean(y_hat == train$Species)
})
max(accuracy)

best_cutoff_length <- cutoff[which.max(accuracy)]
best_cutoff_length
```
The best cutoff for the Petal.Length variable is 4.7. 

Petal.Width
```{r}
y_hat <- sample(c("virginica", "versicolor"), length(test_index), replace = TRUE) 

cutoff <- seq(1.0, 2.5, 0.1)
accuracy <- map_dbl(cutoff, function(x){
  y_hat <- ifelse(train$Petal.Width > x, "virginica", "versicolor") %>%
          factor(levels = levels(train$Species))
        mean(y_hat == train$Species)
})

max(accuracy)
# Max overall accuracy 
best_cutoff_width <- cutoff[which.max(accuracy)]
best_cutoff_width
```
The best cutoff for the Petal.Width variable is 1.5. 

Report overall accuracy predicting on the test set by predicting `virginica` is Petal.Length is greater than the `best_cutoff_length` OR Petal-Width is greater than the `best_cutoff_width`. 
```{r}
y_hat <- ifelse((test$Petal.Length > best_cutoff_length) | (test$Petal.Width > best_cutoff_length), "virginica", "versicolor") %>%
          factor(levels = levels(test$Species))
        mean(y_hat == test$Species)
```

# 2.2. Conditional probabilities 
Denote conditional probabilities for each class k: 

_Pr (Y = k | X1 = x1, ..., Xp = xp), for k = 1, ..., K_

For any given set of predictors (x), we will predict the class of k with the largest probability amount p1(x), p2(x) all the way up to p capital Kx. 

--> But we don't know the pk of xs, and so estimating these conditional probabilities can be thought of as the main challenge of machine learning. --> The better the algorithm estimates p hat of kx, the better our predictor is going to be. 

How good our predictor is going to be depend on these two factors: 

1) how close the maximum probability is to 1 (we cannot do anything about this factor - this limits how well even the best possible algorithm can perform)

2) how close our estimate of the probabilities are to the actual probabilities. 

__Important__: defining our prediction by maximizing the probability is not always optimal in practice and depend on the context. --> Sensitivity and specificity may differ in importance in different contexts. 

__BUT:__ Having a good estimate of the conditional probabilties will suffice for us to build an optimal prediction model since we can control specificity and sensitivity however we wish. (Eg. we can simply chnage the cutoff used to predict one class versus another.)

## Conditional expectations and loss function
For binary data: we can think of conditional probability of y equals 1 when x equals x as a proportion of 1s in the stratum of the population for which x equals x. 

--> Man yof the algorithms can be applied both to categorical and continuous data due to the connection between conditional probabilties and conditional expectations. 

Expectations: average of values in the population. In the case where values are 0s and 1s, the expectations is equivalent to the probability. 

Conditional expectations: in most applications, the same observed predictors does not guarantee the same continuous outcome. 

How can one approach be better than another? 

  1) Binary outcomes: sensitivity, specificity, accuracy and F1 can be used as quantifications. 
  
  BUT: not useful for continuous outcomes. 
  
General approach of defining best in machine learning is to define a loss function. 
__Squared loss function:__ it is simply the difference squared between y hat and y.
To see this, we often use the mean squared error. (If the data is binary, the MSE is equivalent to accuracy.)
--> Approach is to minimize the loss function. 

Because the data is usually a random sample, the MSE is a random variable. And so, we try to find algorithms that minimize the MSE on average across many many random samples. 

Equation for this: the expectations of the squared error.

__Other loss funtions:__ absolute value instead of squared errors. But MSE is the most widely used. 

___Importance:___ Of all possible y hats, the conditional expectations of y given x minimizes the expected loss given x. But this is a much harder job than we would think as the function can take any shape and p. 

# Exercises 
```{r}
set.seed(1)
disease <- sample(c(0,1), size=1e6, replace=TRUE, prob=c(0.98,0.02))
test <- rep(NA, 1e6)
test[disease == 0] <- sample(c(0,1), size=sum(disease==0), replace=TRUE, prob=c(0.90,0.10))
test[disease == 1] <- sample(c(0,1), size=sum(disease==1), replace=TRUE, prob=c(0.15, 0.85))
```

We are now going to write code to compute conditional probabilities for being male in the heights dataset. Round the heights to the closest inch. Plot the estimated conditional probability  P(x) = P(Male | Height = x) for a given x. 

```{r}
library(dslabs)
data("heights")
heights %>% 
	mutate(height = round(height)) %>%
	group_by(height) %>%
	summarize(p = mean(sex == "Male")) %>%
	qplot(height, p, data =.)
```

--> high variability for low values of height

This time use the quantile  0.1,0.2,…,0.9  and the cut function to assure each group has the same number of points.
```{r}
ps <- seq(0, 1, 0.1)
heights %>% 
  mutate(g = cut(height, quantile(height, ps), include.lowest = TRUE)) %>%
	group_by(g) %>%
	summarize(p = mean(sex == "Male"), height = mean(height)) %>%
	qplot(height, p, data =.)
```

```{r include=FALSE}
#install.packages("MASS")
library(MASS)
```


generate data from a bivariate normal distrubution
```{r}
Sigma <- 9*matrix(c(1,0.5,0.5,1), 2, 2)
dat <- MASS::mvrnorm(n = 10000, c(69, 69), Sigma) %>%
	data.frame() %>% setNames(c("x", "y"))
plot(dat)
```

estimate the conditional expectations and make a plot
```{r}
ps <- seq(0, 1, 0.1)
dat %>% 
  mutate(g = cut(x, quantile(x, ps), include.lowest = TRUE)) %>%
  group_by(g) %>%
  summarize(y = mean(y), x = mean(x)) %>%
	qplot(x, y, data =.)
```




